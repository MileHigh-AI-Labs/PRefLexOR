{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70292fb7-04e5-4b98-b240-030915e18869",
   "metadata": {},
   "source": [
    "# PRefLexOR Sample Training Script"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fdd414ea-a4dc-4207-aa56-3e2241529c31",
   "metadata": {
    "tags": []
   },
   "source": [
    "!pip install git+https://github.com/lamm-mit/PRefLexOR.git --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92b8a4d-1631-4d48-9495-6a7403795ad1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import openai\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import datasets\n",
    "import multiprocessing\n",
    "from accelerate import PartialState\n",
    "from trl import ModelConfig, DPOConfig, DPOTrainer, ORPOConfig\n",
    "from transformers import (\n",
    "    TrainingArguments,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    PreTrainedModel,\n",
    "    PreTrainedTokenizerBase,\n",
    "    Trainer,\n",
    "    TrainerCallback,\n",
    "    AutoConfig,\n",
    "    BitsAndBytesConfig,\n",
    "    DataCollator,\n",
    "    is_wandb_available\n",
    ")\n",
    "import torch\n",
    "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
    "from typing import Any, Callable, Dict, List, Literal, Optional, Tuple, Union\n",
    "import inspect\n",
    "import random\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "from contextlib import contextmanager, nullcontext\n",
    "from copy import deepcopy\n",
    "from functools import partial, wraps\n",
    "\n",
    "from llama_index.core import (\n",
    "    VectorStoreIndex, Document, SimpleDirectoryReader, Settings\n",
    ")\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.extractors import TitleExtractor\n",
    "from llama_index.core.ingestion import IngestionPipeline, IngestionCache\n",
    "from llama_index.core.indices.vector_store.retrievers import VectorIndexRetriever\n",
    "\n",
    "from datasets import load_dataset, concatenate_datasets, Dataset\n",
    "\n",
    "# Custom imports\n",
    "from utils import *\n",
    "from active_trainer import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ef9537-26a2-4093-9f1f-f13b563a8618",
   "metadata": {},
   "outputs": [],
   "source": [
    "token = 'hf_.....'\n",
    "from huggingface_hub import login\n",
    "login(token=token)\n",
    "openai_api_key = \"sk-.....\"\n",
    "openai.api_key=openai_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5adf1720-9d40-4fad-8bcd-f947b40e0296",
   "metadata": {},
   "outputs": [],
   "source": [
    "think_start = '<|thinking|>'\n",
    "think_end = '<|/thinking|>'\n",
    "\n",
    "#raw dataset used, data to be expected in 'text' field\n",
    "raw_data= \"lamm-mit/....\"\n",
    "\n",
    "#whether or not to use LoRA to create a trainable model \n",
    "use_LoRA =True\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "25e16641-c332-4b1e-a6a7-3d4c79e1564e",
   "metadata": {},
   "source": [
    "# VLLM\n",
    "\n",
    "vllm serve --port 8000  --gpu-memory-utilization 0.3 --max_model_len 30000 --quantization bitsandbytes --load_format bitsandbytes  meta-llama/Llama-3.1-8B-Instruct\n",
    "\n",
    "# MistralRS\n",
    "\n",
    "~/mistral.rs/target/release/mistralrs-server --port 1234 --isq Q5_1 --no-paged-attn --prefix-cache-n 0  plain -m meta-llama/Llama-3.1-8B-Instruct -a llama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764e9ec0-1e1b-4d16-96be-77350298b854",
   "metadata": {},
   "source": [
    "### Helper functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf4e4f6-09ef-4254-962c-951b2bae8f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(row, manual = True):\n",
    "    \n",
    "    if manual:\n",
    "        row[\"prompt\"] =  f'<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\n{row[\"prompt\"]}<|eot_id|>'\n",
    "        row[\"chosen\"] =  f'<|start_header_id|>assistant<|end_header_id|>\\n\\n{row[\"chosen\"]}<|eot_id|>'\n",
    "        row[\"rejected\"] =f'<|start_header_id|>assistant<|end_header_id|>\\n\\n{row[\"rejected\"]}<|eot_id|>'\n",
    "    else:\n",
    "        row[\"prompt\"] = tokenizer.apply_chat_template   ([ {\"role\": \"user\", \"content\":row[\"prompt\"]}],tokenize=False,add_generation_prompt=False )\n",
    "        row[\"chosen\"] = tokenizer.apply_chat_template   ([ {\"role\": \"assistant\", \"content\":row[\"chosen\"]}],tokenize=False,add_generation_prompt=False )\n",
    "        row[\"rejected\"] = tokenizer.apply_chat_template ( [ {\"role\": \"assistant\", \"content\":row[\"rejected\"]}],tokenize=False,add_generation_prompt=False )\n",
    " \n",
    "    return {'prompt': row[\"prompt\"], 'chosen': row[\"chosen\"], 'rejected': row[\"rejected\"], }\n",
    "\n",
    "\n",
    "def generate_dataset(generate_GPT, index, topics=None, only_include_wrong_answers=False, #if set True, we'll only include wrong answers. \n",
    "                     n_questions_for_each=1,  #how many questions for each topic\n",
    "                     number_nodes_to_get=2,   #how many nodes to make Q-A pair\n",
    "                     \n",
    "                     verbatim=False,get_rejected_from_trained_model=True,model=None, tokenizer=None,\n",
    "                     nodes=None, text=None, process=None):\n",
    "    data = {\"prompt\": [], \"chosen\": [], \"rejected\": [], \"rejected_correct\": []}\n",
    "\n",
    "    if isinstance(topics, list): #topics provided as list of strings\n",
    "        for topic in tqdm(topics):\n",
    "            for _ in range(n_questions_for_each):\n",
    "                try:\n",
    "                    question, correct_response, response_trained_model = get_question_and_answers(generate=generate_GPT, \n",
    "                                                        index = index,\n",
    "                                                        topic=topic, get_rejected_from_trained_model=get_rejected_from_trained_model,\n",
    "                                                        number_nodes_to_get=number_nodes_to_get,  \n",
    "                                                        model=model, tokenizer=tokenizer,\n",
    "                                                    )\n",
    "    \n",
    "                    correct_answer=None\n",
    "                    \n",
    "                    if verbatim:\n",
    "                        print (\"-\"*64)\n",
    "                        print (\">Prompt: \", question)\n",
    "                        print (\">Correct response: \", correct_response)\n",
    "                        print (\">Response model: \", response_trained_model)\n",
    "                        print (\">Correct? \", correct_answer)\n",
    "                    if only_include_wrong_answers:\n",
    "                        correct_answer =False #is_answer_correct(generate_GPT, correct_response, response_trained_model)\n",
    "                    \n",
    "                        if correct_answer==False:\n",
    "                            #do not add if answer is correct\n",
    "                            data[\"prompt\"].append(question)\n",
    "                            data[\"chosen\"].append(correct_response)\n",
    "                            data[\"rejected\"].append(response_trained_model)\n",
    "                            data[\"rejected_correct\"].append (correct_answer)\n",
    "                    else:\n",
    "                        data[\"prompt\"].append(question)\n",
    "                        data[\"chosen\"].append(correct_response)\n",
    "                        data[\"rejected\"].append(response_trained_model)\n",
    "                        data[\"rejected_correct\"].append (correct_answer)\n",
    "                except Exception as e:\n",
    "                    print(f\"An error occurred: {e}\")\n",
    "                    \n",
    "    else: #no topics provided, use random nodes\n",
    "        for _ in tqdm(range(topics)):\n",
    "            for _ in range(n_questions_for_each):\n",
    "                try:\n",
    "                    question, correct_response, response_trained_model = get_question_and_answers(generate=generate_GPT, \n",
    "                                                          index = index, get_rejected_from_trained_model=get_rejected_from_trained_model,\n",
    "                                                          number_nodes_to_get=number_nodes_to_get,  \n",
    "                                                          model=model, tokenizer=tokenizer,\n",
    "                                                    )\n",
    "        \n",
    "                    correct_answer=None\n",
    "                    if verbatim:\n",
    "                        print (\"-\"*64)\n",
    "                        print (\">Prompt: \", question)\n",
    "                        print (\">Correct response: \", correct_response)\n",
    "                        print (\">Response model: \", response_trained_model)\n",
    "                        print (\">Correct? \", correct_answer)\n",
    "                    if only_include_wrong_answers==False:\n",
    "                        #correct_answer=is_answer_correct(correct_response, response_trained_model)\n",
    "                        correct_answer=False#is_answer_correct(generate_GPT, correct_response, response_trained_model)\n",
    "                        if correct_answer==False:\n",
    "                            #do not add if answer is correct\n",
    "                            data[\"prompt\"].append(question)\n",
    "                            data[\"chosen\"].append(correct_response)\n",
    "                            data[\"rejected\"].append(response_trained_model)\n",
    "                            data[\"rejected_correct\"].append (correct_answer)\n",
    "                    else:\n",
    "                        data[\"prompt\"].append(question)\n",
    "                        data[\"chosen\"].append(correct_response)\n",
    "                        data[\"rejected\"].append(response_trained_model)\n",
    "                        data[\"rejected_correct\"].append (correct_answer)\n",
    "               \n",
    "                except Exception as e:\n",
    "                    print(f\"An error occurred: {e}\")                    \n",
    "    \n",
    "    hf_dataset = datasets.Dataset.from_dict(data)\n",
    "    \n",
    "    if process!=None:\n",
    "        hf_dataset = hf_dataset.map(\n",
    "            process,\n",
    "            num_proc=multiprocessing.cpu_count(),\n",
    "        )\n",
    "        \n",
    "    return hf_dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1dba64-57b7-4008-b965-6f697f0b4d7f",
   "metadata": {},
   "source": [
    "### Set up Llama Index for RAG, embeddings, and other tools "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3767786e-07b2-4c06-b6fe-cce0b9bd4046",
   "metadata": {},
   "outputs": [],
   "source": [
    "Settings.embed_model = HuggingFaceEmbedding(\n",
    "    model_name=\"BAAI/bge-large-en-v1.5\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd59092f-630e-4393-90fa-8d6f115cdebb",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load your dataset\n",
    "dataset = datasets.load_dataset(raw_dataset)['train']\n",
    "documents = [Document(text=dataset[i]['text']) for i in range (len (dataset))]\n",
    "\n",
    "pipeline = IngestionPipeline(\n",
    "    transformations=[\n",
    "        SentenceSplitter(chunk_size=1024, chunk_overlap=20),\n",
    "    ],\n",
    "   )\n",
    "nodes = pipeline.run(documents=documents,  show_progress=True,)\n",
    "index = VectorStoreIndex(nodes, show_progress=True,)\n",
    "retriever = index.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3545050f-36d3-4fa9-9c0a-621d92119486",
   "metadata": {},
   "source": [
    "### Load base model and create trainable version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39550e8f-0f9b-418c-b01c-cf780670d66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device='cuda:0'\n",
    "\n",
    "model_name='meta-llama/Llama-3.2-3B-Instruct'\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "try:\n",
    "    del model\n",
    "    del tokenizer\n",
    "except:\n",
    "    print ()\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "try:\n",
    "    del ref_model\n",
    "     \n",
    "except:\n",
    "    print ()\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True,\n",
    "    use_cache=False,\n",
    "     \n",
    "    device_map=\"auto\",\n",
    "    torch_dtype =torch.bfloat16,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    #device_map=\"cuda:0\",\n",
    ").to (device)\n",
    "model.config.use_cache = False\n",
    "\n",
    "model_name_tokenizer='lamm-mit/meta-llama-Meta-Llama-3.2-3B-Instruct-scratchpadtokenizer'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_tokenizer, trust_remote_code=True,\n",
    "                                           use_fast=False,\n",
    "                                         )\n",
    "pad_token='<|finetune_right_pad_id|>'\n",
    "tokenizer.pad_token = pad_token\n",
    "tokenizer.padding_side = \"right\"  \n",
    "\n",
    "if use_LoRA:\n",
    "    from peft import LoraConfig, get_peft_model\n",
    "\n",
    "    lora_alpha = 64\n",
    "    lora_dropout = 0.1\n",
    "    lora_r = 64   \n",
    "    peft_config = LoraConfig(\n",
    "        lora_alpha=lora_alpha,\n",
    "        lora_dropout=lora_dropout,\n",
    "        r=lora_r,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",#[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\", \"gate_proj\"]\n",
    "        target_modules=[\n",
    "            \"q_proj\",\n",
    "            \"k_proj\",\n",
    "            \"v_proj\",\n",
    "            \"o_proj\",\n",
    "            \"gate_proj\",\n",
    "            \"up_proj\",\n",
    "            \"down_proj\",\n",
    "            \"embed_tokens\",\n",
    "            \"lm_head\",\n",
    "        ],\n",
    "    ) \n",
    "    model=get_peft_model(model, peft_config)\n",
    "    \n",
    "    model.print_trainable_parameters()\n",
    "    ref_model=None\n",
    "else:\n",
    "    ref_model=AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        trust_remote_code=True,\n",
    "        use_cache=False,\n",
    "         \n",
    "        device_map=\"auto\",\n",
    "        torch_dtype =torch.bfloat16,\n",
    "        attn_implementation=\"flash_attention_2\",\n",
    "         \n",
    "    ).to (device)\n",
    "    ref_model.config.use_cache = False\n",
    "\n",
    "tokenizer.encode (f'{think_start}{think_end}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82533cf9-128f-4a4e-b163-8d870b6d902b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Setting up default values for the API key, model, and organization\n",
    "generate_GPT_MistralRS = partial(\n",
    "    generate_OpenAI,\n",
    "    openai_api_key='NONE',   \n",
    "    model='meta-llama/Llama-3.1-8B-Instruct',\n",
    "    base_url=\"http://localhost:8000/v1\"\n",
    "   )\n",
    "\n",
    "prompt='What is spider silk?'\n",
    "messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": 'You are a materials scientist.',\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt,\n",
    "            }\n",
    "        ] \n",
    "\n",
    "res, _ = generate_GPT_MistralRS (messages=messages)\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5de82e8-e5f0-4af6-aaa2-ed49fadfd338",
   "metadata": {},
   "source": [
    "### Define functions to generate dataset on-the-fly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343aeb6e-932f-48d7-bddb-be189bd9bcfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "        \n",
    "def get_random_nodes(index, number_nodes_to_get=5):\n",
    "    # Get all nodes from the index\n",
    "    all_nodes = list(index.docstore.docs.values())\n",
    "    \n",
    "    # Ensure we don't request more nodes than exist in the index\n",
    "    number_nodes_to_get = min(number_nodes_to_get, len(all_nodes))\n",
    "    \n",
    "    # Randomly select N nodes\n",
    "    random_nodes = random.sample(all_nodes, number_nodes_to_get)\n",
    "    \n",
    "    # Concatenate their text\n",
    "    concatenated_text = \" \".join([node.text for node in random_nodes])\n",
    "    \n",
    "    return random_nodes, concatenated_text\n",
    "\n",
    "\n",
    "def get_question_and_answers(\n",
    "    generate,\n",
    "    index,\n",
    "    topic=None,\n",
    "    number_nodes_to_get=2,\n",
    "    nodes=None,\n",
    "    text=None,\n",
    "    get_rejected_from_trained_model=True,\n",
    "    model=None,\n",
    "    tokenizer=None,\n",
    "    use_rag=True,\n",
    "    categories=None,  # Categories can be passed in\n",
    "):\n",
    "    \"\"\"\n",
    "    Generates a question and its corresponding correct and rejected answers.\n",
    "\n",
    "    Parameters:\n",
    "    - generate_fn: Function to generate responses based on messages.\n",
    "    - index: The index used for node retrieval and RAG.\n",
    "    - topic: The topic for which to generate the question.\n",
    "    - number_nodes_to_get: Number of nodes to retrieve if topic is provided.\n",
    "    - nodes: Specific nodes to use as context.\n",
    "    - text: Specific text to use as context.\n",
    "    - get_rejected_from_trained_model: Flag to generate a rejected answer using a trained model.\n",
    "    - model: The trained model for generating rejected answers.\n",
    "    - tokenizer: The tokenizer associated with the trained model.\n",
    "    - use_rag: Flag to enable RAG for enhancing the correct answer.\n",
    "    - categories: List of categories to extract; defaults to standard values if None.\n",
    "\n",
    "    Returns:\n",
    "    - question_with_scratchpad: The generated question appended with a scratchpad placeholder.\n",
    "    - correct_response: The enhanced correct answer including the structured scratchpad.\n",
    "    - rejected_answer: The rejected answer.\n",
    "    \"\"\"\n",
    "\n",
    "    # Set default categories if none provided\n",
    "    if categories is None:\n",
    "        categories = [\n",
    "            \"Reasoning Steps\",\n",
    "            \"Relevant Materials or Concepts\",\n",
    "            \"Design Principles\",\n",
    "            \"Material Properties\",\n",
    "            #\"Contradictions or Counterexamples\",\n",
    "            \"Hypothesis\",\n",
    "            #\"Unique Facts\",\n",
    "            #\"Philosophical Aspects\",\n",
    "            #\"Logical Considerations\",\n",
    "        ]\n",
    "\n",
    "    # Retrieve or set the concatenated context text\n",
    "    context = retrieve_context(index, topic, number_nodes_to_get, nodes, text)\n",
    "\n",
    "    # Generate the question\n",
    "    question = generate_question(generate, context)\n",
    "\n",
    "    # Enhance the context using RAG if enabled\n",
    "    enriched_context = (\n",
    "        enhance_context_with_rag(index, question, context) if use_rag else context\n",
    "    )\n",
    "\n",
    "    # Extract information for each category\n",
    "    extracted_info = extract_categories(generate, categories, question, enriched_context,\n",
    "                                        add_RAG=True,\n",
    "                                        #index\n",
    "                                       )\n",
    "\n",
    "    # Generate reasoning steps based on the extracted information\n",
    "    reasoning_steps=''\n",
    "    # Assemble the single structured scratchpad\n",
    "    scratchpad = assemble_scratchpad(extracted_info, reasoning_steps)\n",
    "\n",
    "    # Generate the correct answer\n",
    "    correct_response = generate_correct_answer(\n",
    "        generate, question, enriched_context\n",
    "    )\n",
    "\n",
    "    # Combine the scratchpad and the answer manually\n",
    "    correct_response_with_scratchpad = f\"{scratchpad}\\n{correct_response}\"\n",
    "\n",
    "    # Prepare the question (no need to instruct about scratchpad inclusion)\n",
    "    question_with_scratchpad = question\n",
    "\n",
    "    # Generate the rejected (incorrect) response\n",
    "    rejected_answer = generate_rejected_answer(\n",
    "        generate, question, get_rejected_from_trained_model, model, tokenizer\n",
    "    )\n",
    "\n",
    "    return question_with_scratchpad+f' Use {think_start}.', correct_response_with_scratchpad, rejected_answer\n",
    "\n",
    "\n",
    "def retrieve_context(index, topic, number_nodes_to_get, nodes, text):\n",
    "    if nodes is None and text is None:\n",
    "        if topic:\n",
    "            _, concatenated_text = get_nodes_for_topic(index, topic, number_nodes_to_get)\n",
    "        else:\n",
    "            _, concatenated_text = get_random_nodes(index, number_nodes_to_get)\n",
    "    else:\n",
    "        concatenated_text = text or \" \".join([node.text for node in nodes])\n",
    "    token_length=len(tokenizer.encode(concatenated_text))\n",
    "    print (\"Token length of tokenized node data: \", token_length)\n",
    "    return concatenated_text\n",
    "\n",
    "\n",
    "def generate_question(generate_fn, context):\n",
    "    question_gen_query = (\n",
    "        \"You are a Teacher/Professor. Your task is to setup \"\n",
    "        \"a quiz/examination. Using information in the provided context, formulate \"\n",
    "        \"a single question that captures an important fact from the \"\n",
    "        \"context. Restrict the question to the context information provided, \"\n",
    "        \"and make sure this is a question that a highly trained domain expert can answer without seeing the context.\\n\\n\"\n",
    "        \"Just return the question, nothing else. Do not refer to the context, a paper, names, or authors, just ask the question. \"\n",
    "        \"Do not refer to names, persons, or specific text/context. \"\n",
    "        \"The question must be challenging, deep, and stand on its own and query facts and expert domain knowledge. \"\n",
    "        \"The question must NOT refer to a study, or paper, or a specific author.\"\n",
    "    )\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": 'You are a materials scientist.'},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"{question_gen_query}\\n\\nContext: {context}\",\n",
    "        },\n",
    "    ]\n",
    "    question, _ = generate_fn(messages=messages, temperature=0.7)\n",
    "    return question.strip()\n",
    "\n",
    "\n",
    "def enhance_context_with_rag(index, question, context):\n",
    "    query_engine = index.as_query_engine()\n",
    "    answer = query_engine.query(f\"{question}\\n\\nProvide detailed context and reasoning.\")\n",
    "    enriched_context = f\"{context}\\nAdditional Information: {answer.response}\"\n",
    "    return enriched_context\n",
    "\n",
    "\n",
    "def extract_categories(generate_fn, categories, question, context, index=None, add_RAG=False):\n",
    "    extracted_info = {}\n",
    "    for category in categories:\n",
    "        prompt = f\"\"\"\n",
    "Based on the context, extract the \"{category}\" relevant to the question, keep it brief.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Provide only the \"{category}\" without additional explanations.\n",
    "\n",
    "If you cannot find any, respond with an empty string. Keep the answer brief, but use step-by-step reasoning and a clear explanation. \n",
    "\n",
    "Just provide the answer, do not refer to the context.\n",
    "\"\"\"\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": 'You are a helpful assistant who provides well-reasoned, but succinct responses. Act like a professor.'},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ]\n",
    "        response, _ = generate_fn(messages=messages)\n",
    "        if index != None:\n",
    "            prompt = f\"\"\"\n",
    "Use the draft and improve the \"{category}\" relevant to the question.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Draft: {response}\n",
    "\n",
    "Provide only the \"{category}\" without additional explanations.\n",
    "\n",
    "If you cannot find any, respond with an empty string. Keep the answer brief, but use step-by-step reasoning and a clear explanation. \n",
    "\n",
    "Just provide the answer, do not refer to the context.\n",
    "\"\"\"\n",
    "            response_RAG = query_engine.query(f\"{prompt}\")\n",
    "            print (\"Improve category with RAG\")\n",
    "            response=response_RAG\n",
    "        extracted_info[category] = response.strip()\n",
    "\n",
    "    if add_RAG:\n",
    "        prompt = f\"\"\"\n",
    "Give additional background relevant for answering the question\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Do not answer the question, just provide additional relevant information and insights. Keep it brief.\n",
    "\"\"\"\n",
    "        extracted_info['Additional Background']  = query_engine.query(f\"{prompt}\").response.strip()\n",
    "        \n",
    "    return extracted_info\n",
    "\n",
    "def assemble_scratchpad(extracted_info, reasoning_steps):\n",
    "    scratchpad = f\"{think_start}\\n\"\n",
    "    for category, content in extracted_info.items():\n",
    "        scratchpad += f\"**{category}**:\\n\\n{content}\\n\\n\"\n",
    "    scratchpad += f\"{think_end}\"\n",
    "    return scratchpad\n",
    "\n",
    "\n",
    "def generate_correct_answer(generate_fn, question, context):\n",
    "    prompt = f\"\"\"\n",
    "Using the context provided, answer the following question:\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Provide a comprehensive and accurate answer.\n",
    "\"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": 'You are a knowledgeable materials scientist.'},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "    answer, _ = generate_fn(messages=messages)\n",
    "    return answer.strip()\n",
    "\n",
    "\n",
    "def generate_rejected_answer(generate_fn, question, get_from_trained_model, model, tokenizer):\n",
    "    if get_from_trained_model:\n",
    "        # Implement logic to generate using the trained model\n",
    "        incorrect_answer, _ = generate_local_model(\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            prompt=f\"You provide the answer to: {question}\\nThe answer is:\",\n",
    "            system_prompt='You are a materials scientist.',\n",
    "        )\n",
    "    else:\n",
    "        prompt = f\"\"\"\n",
    "You are to provide an incorrect answer to the question below.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Do not include any reasoning or refer back to the question.\n",
    "\n",
    "Just provide the incorrect answer.\n",
    "\"\"\"\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": 'You are a materials scientist.'},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ]\n",
    "        incorrect_answer, _ = generate_fn(messages=messages)\n",
    "    return incorrect_answer.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea420ff6-0f36-4b97-965e-144c7cdcc268",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Example usage of the function\n",
    "question, correct_ans, rejected_ans=get_question_and_answers(generate=generate_GPT_MistralRS, model=model,\n",
    "                                                             tokenizer=tokenizer, index=index, topic=None, \n",
    "                                                              \n",
    "                             number_nodes_to_get=3, nodes=None, text=None, \n",
    "                             get_rejected_from_trained_model=True,\n",
    "                                                            )\n",
    "\n",
    "print(\"Question:\")\n",
    "print(question)\n",
    "print(\"\\nCorrect Answer with THINKING:\")\n",
    "print(correct_ans)\n",
    "print(\"\\nRejected Answer:\")\n",
    "print(rejected_ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0321b119-428b-4180-aeea-11854efc07a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Setting up default values for the API key, model, and organization\n",
    "generate_GPT_local = partial(\n",
    "    generate_local_model,\n",
    "    model=model, tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "prompt=f'What is spider silk? Use a {think_start}.'\n",
    "messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": 'You are a materials scientist.',\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt,\n",
    "            }\n",
    "        ] \n",
    "\n",
    "res, _ = generate_GPT_local (messages=messages,max_new_tokens=128, #prepend_response=f'<|thinking|>',\n",
    "                            )\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53bc3678-4045-42f6-8286-ab400706a159",
   "metadata": {},
   "source": [
    "### Phase I: Structured Thought Integration Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7aef3c9-5086-4d73-a8ca-c33145b6d933",
   "metadata": {},
   "outputs": [],
   "source": [
    "FT_model_name='Phase_I_results'\n",
    "model_current=FT_model_name+'_'\n",
    "\n",
    "#How many topics are generated in each training step\n",
    "topics= 50\n",
    "\n",
    "#How many questions per topic \n",
    "num_questions_per_topic=1\n",
    "\n",
    "#How many epochs trained in each training step\n",
    "num_epochs_per_dataset_generation=3\n",
    "\n",
    "max_prompt_length=512\n",
    "max_length=1024\n",
    "\n",
    "###############################################\n",
    "cfg =  ORPOConfig(\n",
    "    output_dir=FT_model_name,               # usual HF Trainer args: https://huggingface.co/docs/transformers/main_classes/trainer#transformers.Trainer.args\n",
    "    num_train_epochs=1,                     # number of training epochs\n",
    "    per_device_train_batch_size=1,          # batch size per device during training\n",
    "    gradient_accumulation_steps=2,          # number of steps before performing a backward/update pass\n",
    "    gradient_checkpointing=True,            # use gradient checkpointing to save memory\n",
    "    optim=\"adamw_torch_fused\",              # use fused adamw optimizer\n",
    "    logging_steps=10,                       # log every .. steps\n",
    "    bf16=True,                              # use bfloat16 precision\n",
    "    #tf32=True,                             # use tf32          \n",
    "    learning_rate=5e-5,                     # learning rate\n",
    "    warmup_ratio=0,\n",
    "    warmup_steps=0,\n",
    "    #lr_scheduler_type=\"cosine\",\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    max_prompt_length=max_prompt_length,\n",
    "    remove_unused_columns=False,\n",
    "    max_length=max_length,\n",
    "    beta=0.1,                               # ORPO beta\n",
    "    save_total_limit=3,                     # args related to saving the model...\n",
    "    #save_strategy=\"epoch\",\n",
    "    save_strategy=\"no\",\n",
    "    #push_to_hub=True,                       \n",
    "    hub_private_repo=True,\n",
    "    #report_to=['wandb'],                    # report metrics to Weights & Biases\n",
    "    hub_model_id=f'lamm-mit/{FT_model_name}',\n",
    ")\n",
    "###############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1971cb1f-800b-4fde-970b-cee05dbce23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if isinstance(topics, list) and all(isinstance(t, str) for t in topics):\n",
    "    n_steps = len(topics) * num_questions_per_topic*num_epochs_per_dataset_generation\n",
    "else:\n",
    "    n_steps = topics * num_questions_per_topic*num_epochs_per_dataset_generation\n",
    "\n",
    "trainer = PRefLexORORPOTrainer(\n",
    "         model=model,\n",
    "         args=cfg,\n",
    "         train_dataset=None,\n",
    "         #train_dataset=temp,\n",
    "         tokenizer=tokenizer,\n",
    "         n_steps=n_steps,  # Train for 50 steps before updating dataset\n",
    "         #topics=topics,\n",
    "         topics=topics,\n",
    "         number_nodes_to_get=3,\n",
    "         n_questions_for_each=num_questions_per_topic,\n",
    "         only_include_wrong_answers=False, \n",
    "         process=process,\n",
    "         generate_dataset=generate_dataset,\n",
    "         generate=generate_GPT_MistralRS,  #generate_GPT_OpenAI,\n",
    "         index=index,\n",
    "         get_rejected_from_trained_model=True,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a70355-2aee-40ab-8fc1-4e1b41307588",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "system_prompt='You are a materials scientist.'\n",
    "\n",
    "# Training loop\n",
    "num_iterations = 50\n",
    "for iteration in range(num_iterations):\n",
    "    print(f\"Starting iteration {iteration + 1}/{num_iterations}\")\n",
    "    \n",
    "    # Train for N steps\n",
    "    trainer.train()\n",
    "    \n",
    "    print (64*\"#\") \n",
    "    txt='Tell me why hierarchical structures work so well.'+f' Use {think_start}.'\n",
    "    #txt='What is the reported work of fracture of the nacre in the abalone shell compared to its mineral constituent?'\n",
    "    output_text, _ =generate_local_model (model=model, tokenizer=tokenizer,  prompt=txt,\n",
    "                                               system_prompt=system_prompt,   prepend_response=f'{think_start}',\n",
    "                                   num_return_sequences=1,  repetition_penalty=1.0, #top_p=top_p, top_k=top_k,  \n",
    "                                   temperature=.1,max_new_tokens=1024, messages = [], do_sample=True,\n",
    "                                   )\n",
    "    print (output_text)\n",
    "    print (64*\"-\")\n",
    "    txt=f'What is the relationship between materials and music?'+f' Use {think_start}.'\n",
    "    #txt='What is the reported work of fracture of the nacre in the abalone shell compared to its mineral constituent?'\n",
    "    output_text, messages =generate_local_model (model=model, tokenizer=tokenizer,  prompt=txt,\n",
    "                                               system_prompt=system_prompt,  prepend_response=f'{think_start}',\n",
    "                                   num_return_sequences=1,  repetition_penalty=1.0, #top_p=top_p, top_k=top_k,  \n",
    "                                   temperature=.1,max_new_tokens=1024, messages = [], do_sample=True,\n",
    "                                   )\n",
    "    print (output_text)\n",
    "    print (64*\"-\")\n",
    "    txt='Tell me why hierarchical structures work so well.'+f' Use {think_start}.'\n",
    "    #txt='What is the reported work of fracture of the nacre in the abalone shell compared to its mineral constituent?'\n",
    "    output_text, messages =generate_local_model (model=model, tokenizer=tokenizer,  prompt=txt,\n",
    "                                               system_prompt=system_prompt,  prepend_response=f'{think_start}',\n",
    "                                   num_return_sequences=1,  repetition_penalty=1.0, #top_p=top_p, top_k=top_k,  \n",
    "                                   temperature=.1,max_new_tokens=1024, messages = [], do_sample=True,\n",
    "                                   )\n",
    "    print (output_text)\n",
    "    print (64*\"#\")\n",
    "    \n",
    "    trainer.update_dataset()\n",
    "    \n",
    "    trainer.save_model(f\"./{model_current}\")\n",
    "    model.push_to_hub (f\"lamm-mit/{model_current}\", private=True)\n",
    "    tokenizer.push_to_hub (f\"lamm-mit/{model_current}\", private=True)\n",
    "\n",
    "    print(f\"Completed iteration {iteration + 1}/{num_iterations}\")\n",
    "\n",
    "    #trainer.update_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3164528-9d0e-4e5e-8f71-2956f52e6647",
   "metadata": {},
   "source": [
    "### Phase II: Independent Reasoning Development\n",
    "\n",
    "#### First, merge adapter into base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845d079d-4e91-4b59-b78b-c1fb2c6a6bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "import gc \n",
    "try:\n",
    "    del model\n",
    "    del tokenizer\n",
    "    del merged_model\n",
    "except:\n",
    "    print ()\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "try:\n",
    "    del ref_model\n",
    "     \n",
    "except:\n",
    "    print ()\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "model_name=\"lamm-mit/meta-llama-Llama-3.2-3B-Instruct-untied\"\n",
    "model_base = AutoModelForCausalLM.from_pretrained(model_name,     torch_dtype =torch.bfloat16,\n",
    "    attn_implementation=\"flash_attention_2\",device_map=\"auto\",trust_remote_code=True,\n",
    "    \n",
    "     )\n",
    " \n",
    "peft_model_id = model_current\n",
    "\n",
    "model_base = PeftModel.from_pretrained(model_base, peft_model_id)\n",
    "\n",
    "model = model_base.merge_and_unload()\n",
    "\n",
    "model_name_tokenizer='lamm-mit/meta-llama-Meta-Llama-3.2-3B-Instruct-scratchpadtokenizer'\n",
    "#model_name_tokenizer=model_name\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_tokenizer, trust_remote_code=True,\n",
    "                                           use_fast=False,\n",
    "                                         )\n",
    "pad_token='<|finetune_right_pad_id|>'\n",
    "tokenizer.pad_token = pad_token\n",
    "tokenizer.padding_side = \"right\"  \n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "try:\n",
    "    del model_base\n",
    "     \n",
    "except:\n",
    "    print ()\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db03a16a-8648-47a8-bb67-b66f86c07237",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_LoRA:\n",
    "\n",
    "    from peft import LoraConfig, get_peft_model\n",
    "    lora_alpha = 64\n",
    "    lora_dropout = 0.1\n",
    "    lora_r = 64\n",
    "\n",
    "    peft_config = LoraConfig(\n",
    "        lora_alpha=lora_alpha,\n",
    "        lora_dropout=lora_dropout,\n",
    "        r=lora_r,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\", \n",
    "        target_modules=[\n",
    "            \"q_proj\",\n",
    "            \"k_proj\",\n",
    "            \"v_proj\",\n",
    "            \"o_proj\",\n",
    "            \"gate_proj\",\n",
    "            \"up_proj\",\n",
    "            \"down_proj\",\n",
    "            #\"embed_tokens\",\n",
    "            #\"lm_head\",\n",
    "        ], \n",
    "      #  modules_to_save=[\"embed_tokens\", \"lm_head\"]\n",
    "    ) \n",
    "    \n",
    "    from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
    "    \n",
    "    #model = prepare_model_for_kbit_training(model)\n",
    "    model=get_peft_model(model, peft_config)\n",
    "    \n",
    "    model.print_trainable_parameters()\n",
    "    ref_model=None\n",
    "else:\n",
    "    print (\"We will not use LoRA\")\n",
    "\n",
    "    ref_model=AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        trust_remote_code=True,\n",
    "        use_cache=False,\n",
    "         \n",
    "        device_map=\"auto\",\n",
    "        torch_dtype =torch.bfloat16,\n",
    "        attn_implementation=\"flash_attention_2\",\n",
    "        #device_map=\"cuda:0\",\n",
    "    ).to (device)\n",
    "    ref_model.config.use_cache = False\n",
    "\n",
    "tokenizer.encode ('<|thinking|><|/thinking|><|scratchpad|><|/scratchpad|><|reflect|><|/reflect|><|response|><|/response|>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac9d4bb-5e08-46b3-bc34-9d40d0febe4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up default values for the API key, model, and organization\n",
    "generate_GPT_local = partial(\n",
    "    generate_local_model,\n",
    "    model=model, tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "prompt=f'What is spider silk? Use a {think_start}.'\n",
    "messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": 'You are a materials scientist.',\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt,\n",
    "            }\n",
    "        ] \n",
    "\n",
    "res, _ = generate_GPT_local (messages=messages,max_new_tokens=128, #prepend_response=f'<|thinking|>',\n",
    "                            )\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81cf7150-e33b-431d-af44-f19624200e40",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "txt='Tell me why hierarchical structures work so well.'+f' Use {think_start}.'\n",
    "\n",
    "output_text, _ =generate_local_model (model=model, tokenizer=tokenizer,  prompt=txt,\n",
    "                                           system_prompt='',   #prepend_response=f'{think_start}',\n",
    "                               num_return_sequences=1,  repetition_penalty=1.0, #top_p=top_p, top_k=top_k,  \n",
    "                               temperature=.01,max_new_tokens=1024, messages = [], do_sample=True,\n",
    "                               )\n",
    "print (output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58ec3df-f389-45cf-beb0-a10ee7b18d34",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "#### Set up PRefLexORDPOTrainer for Phase II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2692414-5912-48ee-a31b-e7a6f3948fa0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "FT_model_name='Phase_II_results'\n",
    "model_current=FT_model_name+'_'\n",
    "\n",
    "max_prompt_length=512\n",
    "max_length=1024\n",
    "\n",
    "class RewardLoggingCallback(TrainerCallback):\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if state.log_history:\n",
    "            try:\n",
    "                print ( f\"Step={state.log_history[-1]['step'] }\", \"rewards/margins=\", state.log_history[-1]['rewards/margins'] ,\n",
    "                      \"loss=\", state.log_history[-1]['loss'],   \"rewards/accuracy=\", state.log_history[-1]['rewards/accuracies'] )# Get the last log entry\n",
    "            except:\n",
    "                print (end='')\n",
    " \n",
    "cfg = DPOConfig(\n",
    "    output_dir=FT_model_name,     # usual HF Trainer args: https://huggingface.co/docs/transformers/main_classes/trainer#transformers.Trainer.args\n",
    "    num_train_epochs=1,                     # number of training epochs\n",
    "    per_device_train_batch_size=1,          # batch size per device during training\n",
    "    gradient_accumulation_steps=2,          # number of steps before performing a backward/update pass\n",
    "    gradient_checkpointing=False,            # use gradient checkpointing to save memory\n",
    "    optim=\"adamw_torch_fused\",              # use fused adamw optimizer\n",
    "    logging_steps=10,                       # log every .. steps\n",
    "    bf16=True,                              # use bfloat16 precision\n",
    "    #tf32=True,                             # use tf32          \n",
    "    max_grad_norm=0.3,\n",
    "    learning_rate=1e-6,                     # learning rate\n",
    "    warmup_ratio=0,\n",
    "    warmup_steps=0,\n",
    "    #lr_scheduler_type=\"cosine\",\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    max_prompt_length=512,\n",
    "    max_length=2000,\n",
    "    remove_unused_columns=False,\n",
    "    #max_length=1024,\n",
    "    beta=0.5,                               # ORPO beta\n",
    "    save_total_limit=50,                     # args related to saving the model...\n",
    "    save_strategy=\"epoch\",\n",
    "    #push_to_hub=True,                       \n",
    "    hub_private_repo=True,\n",
    "    report_to=['none'],                    # report metrics to Weights & Biases\n",
    "    hub_model_id=f'lamm-mit/{FT_model_name}',\n",
    "    loss_type=\"exo_pair\",                  # Loss type for DPO\n",
    "    label_smoothing=5e-3,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f0ffa6-766e-4433-b265-4a1bd1c43181",
   "metadata": {},
   "source": [
    "#### Define new function to generate rejected answer using current model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479265fe-c875-46cb-8b1f-c984b6ae6327",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_rejected_answer(generate_fn, question, get_from_trained_model, model, tokenizer):\n",
    "    \n",
    "    if get_from_trained_model:\n",
    "    incorrect_answer, _ = generate_local_model(\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            prompt=f\"{question}\"+f' Use {think_start}.',\n",
    "            system_prompt='You are a materials scientist.', max_new_tokens=1500,\n",
    "        )\n",
    "    else:\n",
    "        prompt = f\"\"\"\n",
    "You are to provide an incorrect answer to the question below.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Do not include any reasoning or refer back to the question.\n",
    "\n",
    "Just provide the incorrect answer.\n",
    "\"\"\"\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": 'You are a materials scientist.'},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ]\n",
    "        incorrect_answer, _ = generate_fn(messages=messages)\n",
    "    return incorrect_answer.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419cdac3-003f-4f64-ae5d-ba3c116b2b66",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Example usage  \n",
    "\n",
    "question, correct_ans, rejected_ans=get_question_and_answers(generate=generate_GPT_MistralRS, model=model,\n",
    "                                                             tokenizer=tokenizer, index=index, topic=None, #'spider silk strength', \n",
    "                             number_nodes_to_get=3, nodes=None, text=None,  \n",
    "                             get_rejected_from_trained_model=True,\n",
    "                                                            )\n",
    "\n",
    "print(\"Question:\")\n",
    "print(question)\n",
    "print(\"\\nCorrect Answer with THINKING:\")\n",
    "print(correct_ans)\n",
    "print(\"\\nRejected Answer:\")\n",
    "print(rejected_ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b140406-05b9-4938-81ac-92964f1a8245",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "\n",
    "topics= 50\n",
    "num_questions_per_topic=1\n",
    "num_epochs_per_dataset_generation=1\n",
    "\n",
    "if isinstance(topics, list) and all(isinstance(t, str) for t in topics):\n",
    "    n_steps = len(topics) * num_questions_per_topic*num_epochs_per_dataset_generation\n",
    "else:\n",
    "    n_steps = topics * num_questions_per_topic*num_epochs_per_dataset_generation\n",
    "\n",
    "trainer = PRefLexORDPOTrainer(\n",
    "        model=model,\n",
    "        ref_model=ref_model, # set to none if use LoRA\n",
    "        args=cfg,\n",
    "\n",
    "        train_dataset=None, \n",
    "        tokenizer=tokenizer,\n",
    "        n_steps=n_steps,  # Train for 50 steps before updating dataset\n",
    "        topics=topics,\n",
    "        number_nodes_to_get=3,\n",
    "        n_questions_for_each=num_questions_per_topic,\n",
    "        only_include_wrong_answers=False, \n",
    "        process=process,\n",
    "    generate_dataset=generate_dataset,\n",
    "    generate=generate_GPT_MistralRS, #generate_GPT_OpenAI,\n",
    "    get_rejected_from_trained_model=True,\n",
    "    index=index,\n",
    "\n",
    "    #OPTION 1: Only include answer in loss\n",
    "    dynamic_answer_comparison = True,  \n",
    "    \n",
    "    #OPTION 2: Mask out all sections of thinking, to a degree (percentage indicates how much is randomly masked out)\n",
    "    mask_thinking_tokens = False, \n",
    "    thinking_token_mask_percentage = .2,  # Default to masking 100% of thinking tokens\n",
    "\n",
    "    think_start_token= '<|thinking|>', think_end_token= '<|/thinking|>',\n",
    "    include_thinking_token_in_labels=True,\n",
    "    callbacks=[RewardLoggingCallback()],\n",
    "\n",
    "    )\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90fb3e5a-7b94-497a-bafe-65735ccddf97",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "system_prompt='You are a materials scientist.'\n",
    "\n",
    "# Training loop\n",
    "num_iterations = 50\n",
    "\n",
    "for iteration in range(num_iterations):\n",
    "    print(f\"Starting iteration {iteration + 1}/{num_iterations}\")\n",
    "    \n",
    "    # Train for N steps\n",
    "    trainer.train()\n",
    "        \n",
    "    print (64*\"#\")\n",
    "    # Update dataset\n",
    "    txt='Tell me why hierarchical structures work so well.'#+f' Use {think_start}.'\n",
    "    #txt='What is the reported work of fracture of the nacre in the abalone shell compared to its mineral constituent?'\n",
    "    output_text, _ =generate_local_model (model=model, tokenizer=tokenizer,  prompt=txt,\n",
    "                                               system_prompt=system_prompt,   #prepend_response=f'{think_start}',\n",
    "                                   num_return_sequences=1,  repetition_penalty=1.0, #top_p=top_p, top_k=top_k,  \n",
    "                                   temperature=.1,max_new_tokens=1024, messages = [], do_sample=True,\n",
    "                                   )\n",
    "    print (output_text)\n",
    "    print (64*\"-\")\n",
    "    #trainer.update_dataset()\n",
    "    txt='Tell me why hierarchical structures work so well.'+f' Use {think_start}.'\n",
    "    #txt='What is the reported work of fracture of the nacre in the abalone shell compared to its mineral constituent?'\n",
    "    output_text, _ =generate_local_model (model=model, tokenizer=tokenizer,  prompt=txt,\n",
    "                                               system_prompt=system_prompt,   prepend_response=f'{think_start}',\n",
    "                                   num_return_sequences=1,  repetition_penalty=1.0, #top_p=top_p, top_k=top_k,  \n",
    "                                   temperature=.1,max_new_tokens=1024, messages = [], do_sample=True,\n",
    "                                   )\n",
    "    print (output_text)\n",
    "    print (64*\"-\")\n",
    "    txt='Explain the relationship between materials and music.'+f' Use {think_start}.'\n",
    "    #txt='What is the reported work of fracture of the nacre in the abalone shell compared to its mineral constituent?'\n",
    "    output_text, _ =generate_local_model (model=model, tokenizer=tokenizer,  prompt=txt,\n",
    "                                               system_prompt=system_prompt,   prepend_response=f'{think_start}',\n",
    "                                   num_return_sequences=1,  repetition_penalty=1.0, #top_p=top_p, top_k=top_k,  \n",
    "                                   temperature=.1,max_new_tokens=1024, messages = [], do_sample=True,\n",
    "                                   )\n",
    "    print (output_text) \n",
    "    print (64*\"#\")\n",
    "    \n",
    "    trainer.update_dataset()\n",
    "    \n",
    "    print(f\"Completed iteration {iteration + 1}/{num_iterations}\")\n",
    "\n",
    "    trainer.save_model(f\"./{model_current}\")\n",
    "    model.push_to_hub (f\"lamm-mit/{model_current}\", private=True)\n",
    "    tokenizer.push_to_hub (f\"lamm-mit/{model_current}\", private=True)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd87c00-8ef1-4150-9f61-414741795eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(f\"./{model_current}\")\n",
    "model.push_to_hub (f\"lamm-mit/{model_current}\", private=True)\n",
    "tokenizer.push_to_hub (f\"lamm-mit/{model_current}\", private=True)    "
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m123",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m123"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
